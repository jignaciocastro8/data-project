{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "662d3da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jigna\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "\n",
    "#Visualizations\n",
    "#import plotly.express as px\n",
    "#import seaborn as sns\n",
    "#import pyLDAvis.gensim\n",
    "#import chart_studio\n",
    "#import chart_studio.plotly as py \n",
    "#import chart_studio.tools as tls\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "#data_path = \"data/santiago_covid_2020.csv\"\n",
    "#data_path = \"data/santiago_enero.csv\"\n",
    "data_path = \"data/gabrielboric.csv\"\n",
    "\n",
    "\n",
    "\n",
    "# spanish stop words\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "stop_words = ['t', 'si', 'q', 'https', 'co', 'solo', 'ser', 'bien', \n",
    "            'así', 'ma', 'mas', 'igual', 'va', 'después',\n",
    "            'hacer', 'hace', 'creo'] + list(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6ef228",
   "metadata": {},
   "source": [
    "# **Métodos útiles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1e9ff06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hola', 'chao']\n"
     ]
    }
   ],
   "source": [
    "def delete_chars(text, unwanted_chars):\n",
    "    \"\"\"\n",
    "    useful method to replace a list of chars on text.\n",
    "    return:\n",
    "        (str) the same str without the chars in chars.\n",
    "    \"\"\"\n",
    "    for char in unwanted_chars:\n",
    "        text = text.replace(char, '')\n",
    "    return text\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    \"\"\"\n",
    "    Replaces unwanted characters and performs\n",
    "    a preprocessing.\n",
    "    input: \n",
    "        (str) tweet.\n",
    "    return:\n",
    "        (str[]) final: list of words.\n",
    "    \"\"\"\n",
    "    unwanted = ['#', ',', '.', '!', '?', '¿', '¡', '()',\\\n",
    "                ')', '-', '=', 'jaja', 'jajaja']\n",
    "    final = delete_chars(tweet, unwanted).split()\n",
    "    final = [w.lower() for w in final]\n",
    "    final = [w for w in final if w not in stop_words and len(w) > 3]\n",
    "    # Se eliminan links y @users\n",
    "    final = [w for w in final if w[:4] != 'http']\n",
    "    final = [w for w in final if w[:1] != '@']\n",
    "    return final\n",
    "    \n",
    "# mini mini test\n",
    "print(preprocess_tweet('##hola) ¿chao? =) jaja!'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fc8179",
   "metadata": {},
   "source": [
    "# **Tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cb6d2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jigna\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3072: DtypeWarning: Columns (9,22) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hoy nos reunimos con el Rector de la @uvalpochile @ocorralesj para conversar sobre educación superior y los desafíos de las universidades estatales para el nuevo Chile. Nuestro gobierno buscará fortalecer la #educación pública de calidad.  https://t.co/aXvXMWoNPx'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(data_path)[['date', 'tweet']]\n",
    "data.iloc[0]['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06c662b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reunimos',\n",
       " 'rector',\n",
       " 'conversar',\n",
       " 'educación',\n",
       " 'superior',\n",
       " 'desafíos',\n",
       " 'universidades',\n",
       " 'estatales',\n",
       " 'nuevo',\n",
       " 'chile',\n",
       " 'gobierno',\n",
       " 'buscará',\n",
       " 'fortalecer',\n",
       " 'educación',\n",
       " 'pública',\n",
       " 'calidad']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_tweet(data.iloc[0]['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b7623b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36690    [estatal, cambios, acceso, insuficientes, ajus...\n",
       "38494        [sacar, twitter, quiero, policias, digitales]\n",
       "11184    [conversando, apareció, poetisa, chilena, veró...\n",
       "38008    [cabe, duda, ruiz, tagle, presidente, sólo, hi...\n",
       "11241    [declaración, principios, sigue, piñera, probl...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se pre-procesan los tweets: Esto transforma cada\n",
    "# tweet en una colección de palabras.\n",
    "# processed_tweets corresponde a una lista de listas de palabras.\n",
    "\n",
    "processed_tweets = data['tweet'].map(preprocess_tweet)\n",
    "processed_tweets.head()\n",
    "\n",
    "# Se van a eliminar tweets pequeños: con menos de 5 palabras después\n",
    "# del preprocessing.\n",
    "dropers = []\n",
    "for ind, tweet in enumerate(processed_tweets):\n",
    "    if len(tweet) < 5:\n",
    "        dropers.append(ind) \n",
    "\n",
    "processed_tweets = processed_tweets.drop(dropers)\n",
    "processed_tweets.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d983bef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 buscará\n",
      "1 calidad\n",
      "2 chile\n",
      "3 conversar\n",
      "4 desafíos\n",
      "5 educación\n",
      "Largo del diccionario:  29196\n"
     ]
    }
   ],
   "source": [
    "# Se crea el vocabulario. \n",
    "# Corresponde a crear una lista con todas las palabras involucrada \n",
    "# en el corpus asignando un índice único a cada una.\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(processed_tweets)\n",
    "\n",
    "c = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k,v)\n",
    "    c += 1\n",
    "    if c > 5: break\n",
    "        \n",
    "print('Largo del diccionario: ', len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b950190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet 0:  [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1)]\n",
      "tweet 1:  [(15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Se transforman las palabras a vectores con el dictionary.\n",
    "# bow = \"bag of words\"\n",
    "\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_tweets]\n",
    "print('tweet 0: ', bow_corpus[0])\n",
    "print('tweet 1: ', bow_corpus[1])\n",
    "\n",
    "# Con esto cada tweet se representa como una colección de tuplas (w, a) \n",
    "# donde w es el índice de la palabra y a la cantidad de apariciones en ese\n",
    "# tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c47c22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea y usa el lda model\n",
    "# Running LDA using Bag of Words\n",
    "\n",
    "# LdaMulticore\n",
    "\n",
    "from gensim.models import LdaModel\n",
    "lda_model = LdaModel(bow_corpus, \\\n",
    "                                    num_topics=5,\\\n",
    "                                    id2word=dictionary)\n",
    "\n",
    "\n",
    "# Los hiperparámetros serán calibrados más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab9bd7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.010*\"gracias\" + 0.009*\"abrazo\" + 0.008*\"estudiantil\" + 0.007*\"chile\" + 0.007*\"estudiantes\" + 0.006*\"muchas\" + 0.005*\"izquierda\" + 0.005*\"compa\" + 0.005*\"aguante\" + 0.005*\"autónoma\"\n",
      "Topic: 1 \n",
      "Words: 0.015*\"educación\" + 0.010*\"acuerdo\" + 0.008*\"política\" + 0.007*\"ahora\" + 0.007*\"lucro\" + 0.006*\"(via\" + 0.005*\"gobierno\" + 0.005*\"reforma\" + 0.005*\"vamos\" + 0.005*\"confech\"\n",
      "Topic: 2 \n",
      "Words: 0.009*\"magallanes\" + 0.007*\"ahora\" + 0.007*\"mucha\" + 0.006*\"vamos\" + 0.006*\"fuerza\" + 0.005*\"mañana\" + 0.005*\"semana\" + 0.005*\"reunión\" + 0.005*\"política\" + 0.005*\"buena\"\n",
      "Topic: 3 \n",
      "Words: 0.008*\"universidad\" + 0.008*\"cabros\" + 0.007*\"universitario\" + 0.007*\"senado\" + 0.006*\"arenas\" + 0.006*\"punta\" + 0.004*\"educación\" + 0.004*\"magallanes\" + 0.004*\"chile\" + 0.004*\"ahora\"\n",
      "Topic: 4 \n",
      "Words: 0.014*\"fech\" + 0.009*\"movimiento\" + 0.006*\"chile\" + 0.004*\"izquierda\" + 0.004*\"derecha\" + 0.004*\"parece\" + 0.004*\"buena\" + 0.004*\"cosa\" + 0.004*\"tolerancia0\" + 0.004*\"compañeros\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaeff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar:\n",
    "\n",
    "#pyLDAvis.enable_notebook()\n",
    "#pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "213462d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gensim.models import CoherenceModel\n",
    "# Compute Coherence Score\n",
    "#coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_tweets, dictionary=dictionary, coherence='c_v')\n",
    "#coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598a78b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bea891d4f4659476d21e83a48ee1b355f8820be15a794f539b2195aa6b1473be"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
