{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jigna\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from utils import preprocess_tweet\r\n",
    "import gensim\r\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             date                                              tweet\n",
      "67973  2021-01-04                              @sherlocka007 ü§£ü§£ü§£ü§£ü§£ü§£ü§£\n",
      "68341  2021-01-04             @MConsuelofg La famosa 13 de Dr. House\n",
      "65706  2021-01-05                       Pucha que despert√© contento.\n",
      "14099  2021-01-09                 @vale_ortega @CristianNeira Jajaja\n",
      "31568  2021-01-07  Me cuesta pensar en una instituci√≥n m√°s s√≥rdid...\n",
      "--------------------------------------------------------------------------\n",
      "Cantidad de tweets:  81612\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/seven_days_tweets.csv\")[[\"date\", \"tweet\"]]\r\n",
    "print(data.sample(5))\r\n",
    "\r\n",
    "print(\"--------------------------------------------------------------------------\")\r\n",
    "print(\"Cantidad de tweets: \", len(data))\r\n",
    "print(\"--------------------------------------------------------------------------\")\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [jaja, papito, jugamos, wanderers, dijo:, mat√≠...\n",
       "1     [‚Äúpucha, usada‚Äù, fotos, bicicleta, lucas, ü§¶üèΩ‚Äç‚ôÄ...\n",
       "8     [recuerda, julio, lopez, blanco, agente, civil...\n",
       "11    [ganas, contrato, a√±os, precio, baja, cierto, ...\n",
       "12           [mala, calidad, desenfocado, niun, brillo]\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se pre-procesan los tweets: Esto transforma cada\r\n",
    "# tweet en una colecci√≥n de palabras.\r\n",
    "# processed_tweets corresponde a una lista de listas de palabras.\r\n",
    "\r\n",
    "processed_tweets = data['tweet'].map(preprocess_tweet)\r\n",
    "processed_tweets.head()\r\n",
    "\r\n",
    "# Se van a eliminar tweets peque√±os: con menos de 5 palabras despu√©s\r\n",
    "# del preprocessing.\r\n",
    "dropers = []\r\n",
    "for ind, tweet in enumerate(processed_tweets):\r\n",
    "    if len(tweet) < 5:\r\n",
    "        dropers.append(ind) \r\n",
    "\r\n",
    "processed_tweets = processed_tweets.drop(dropers)\r\n",
    "processed_tweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 jaja\n",
      "1 mat√≠as\n",
      "2 wanderers\n",
      "3 acaso\n",
      "4 bicicleta\n",
      "5 fotos\n",
      "------------------------------------------\n",
      "Largo del diccionario:  3282\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Se crea el vocabulario. \r\n",
    "dictionary = gensim.corpora.Dictionary(processed_tweets)\r\n",
    "# Se quitan palabras que aparecen en menos de 20 tweets y las\r\n",
    "# que aparecen en m√°s del 50% del total de tweets (?).\r\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\r\n",
    "c = 0\r\n",
    "for k, v in dictionary.iteritems():\r\n",
    "    print(k,v)\r\n",
    "    c += 1\r\n",
    "    if c > 5: break\r\n",
    "        \r\n",
    "print(\"------------------------------------------\")\r\n",
    "print('Largo del diccionario: ', len(dictionary))\r\n",
    "print(\"------------------------------------------\")\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet 0:  [(0, 1), (1, 1), (2, 1)]\n",
      "tweet 1:  [(3, 1), (4, 2), (5, 1), (6, 1), (7, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Se transforman las palabras a vectores con el dictionary.\r\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_tweets]\r\n",
    "print('tweet 0: ', bow_corpus[0])\r\n",
    "print('tweet 1: ', bow_corpus[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea y usa el lda model\r\n",
    "from gensim.models import LdaModel\r\n",
    "\r\n",
    "# Set training parameters.\r\n",
    "num_topics = 5\r\n",
    "chunksize = 2000\r\n",
    "passes = 20\r\n",
    "iterations = 400\r\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\r\n",
    "\r\n",
    "# Make a index to word dictionary.\r\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\r\n",
    "id2word = dictionary#.id2token\r\n",
    "\r\n",
    "lda_model = LdaModel(\r\n",
    "    corpus=bow_corpus,\r\n",
    "    id2word=id2word,\r\n",
    "    chunksize=chunksize,\r\n",
    "    alpha='auto',\r\n",
    "    eta='auto',\r\n",
    "    iterations=iterations,\r\n",
    "    num_topics=num_topics,\r\n",
    "    passes=passes,\r\n",
    "    eval_every=eval_every,\r\n",
    "    random_state=1\r\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.062*\"chile\" + 0.052*\"avenida\" + 0.047*\"santiago\" + 0.038*\"sentir\" + 0.038*\"voluntad\" + 0.028*\"fuego\" + 0.024*\"gracias\" + 0.024*\"foto\" + 0.022*\"acaba\" + 0.021*\"publicar\"\n",
      "Topic: 1 \n",
      "Words: 0.028*\"accidente\" + 0.018*\"gran\" + 0.016*\"basura\" + 0.013*\"vamos\" + 0.012*\"partido\" + 0.011*\"vehicular1\" + 0.010*\"jose\" + 0.009*\"general\" + 0.009*\"miguel\" + 0.009*\"pueblo\"\n",
      "Topic: 2 \n",
      "Words: 0.031*\"forma\" + 0.030*\"jadue\" + 0.030*\"delincuentes\" + 0.029*\"guerra\" + 0.028*\"imagino\" + 0.028*\"profesional\" + 0.027*\"premio\" + 0.027*\"renuncia\" + 0.020*\"bien\" + 0.015*\"personas\"\n",
      "Topic: 3 \n",
      "Words: 0.021*\"a√±os\" + 0.019*\"vida\" + 0.016*\"amor\" + 0.016*\"educaci√≥n\" + 0.015*\"dem√°s\" + 0.015*\"seguro\" + 0.015*\"macul\" + 0.015*\"mala\" + 0.015*\"vivir\" + 0.014*\"ni√±a\"\n",
      "Topic: 4 \n",
      "Words: 0.035*\"d√≠as\" + 0.033*\"bueno\" + 0.031*\"buena\" + 0.029*\"falta\" + 0.028*\"dijo\" + 0.028*\"hablar\" + 0.027*\"casi\" + 0.027*\"pas√≥\" + 0.026*\"muerte\" + 0.026*\"p√∫blico\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\r\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bea891d4f4659476d21e83a48ee1b355f8820be15a794f539b2195aa6b1473be"
  },
  "kernelspec": {
   "display_name": "Python 3.6.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}