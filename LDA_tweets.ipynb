{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bd23eefa-e4a7-468b-b90f-53dce1afab86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "#data_path = \"data/santiago_covid_2020.csv\"\n",
    "data_path = \"data/santiago_enero.csv\"\n",
    "\n",
    "\n",
    "# spanish stop words\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "stop_words = ['t', 'si', 'q', 'https', 'co', 'solo', 'ser', 'bien', \n",
    "            'as√≠', 'ma', 'mas', 'igual', 'va', 'despu√©s',\n",
    "            'hacer', 'hace', 'creo'] + list(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46101d0b-2231-4b1b-9496-37026d440839",
   "metadata": {},
   "source": [
    "# **M√©todos √∫tiles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f44bc42b-090b-426c-8787-25c01f8af2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    final = tweet.replace('#', '').replace(',', '').replace('.', '').replace('!', '').replace('?', '').replace('¬ø', '').split()\n",
    "    final = [w.lower() for w in final]\n",
    "    final = [w for w in final if w not in stop_words and len(w) > 3]\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1002dccd-f5c7-443b-8ab0-281b79968821",
   "metadata": {},
   "source": [
    "# **Tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9e89c198-e52c-4069-906a-053e4860e794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10318</th>\n",
       "      <td>2021-01-29</td>\n",
       "      <td>00:59 en Cerrillos 10-2 BX3 Alaska y Espa√±a  h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2717</th>\n",
       "      <td>2021-01-29</td>\n",
       "      <td>Vamo üê∑ #LuzesEternas #Palmeiras</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8910</th>\n",
       "      <td>2021-01-29</td>\n",
       "      <td>Acaba de publicar una foto en Talca,region del...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13105</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>16:25 en La Cisterna 10-3-1 RB5 Angamos y Almi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>2021-01-29</td>\n",
       "      <td>I think i have covid üòò</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             date                                              tweet\n",
       "10318  2021-01-29  00:59 en Cerrillos 10-2 BX3 Alaska y Espa√±a  h...\n",
       "2717   2021-01-29                    Vamo üê∑ #LuzesEternas #Palmeiras\n",
       "8910   2021-01-29  Acaba de publicar una foto en Talca,region del...\n",
       "13105  2021-01-01  16:25 en La Cisterna 10-3-1 RB5 Angamos y Almi...\n",
       "871    2021-01-29                             I think i have covid üòò"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(data_path)[['date', 'tweet']]\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ea3e047c-74a9-4d8c-88cb-efe3d04570b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['will',\n",
       " 'biden',\n",
       " 'fauci',\n",
       " 'take',\n",
       " 'responsibility',\n",
       " 'covid',\n",
       " 'worse',\n",
       " 'blame',\n",
       " 'governors']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_tweet(data.iloc[0]['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "68f9e9b9-f4a9-4ac0-a852-86541b1b3dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [will, biden, fauci, take, responsibility, cov...\n",
       "1          [through, another, year, covid, just, jump]\n",
       "2    [@iamnickreynolds, @mattgaetz, think, about, t...\n",
       "3    [covid, crisis, will, until, massive, fraud, c...\n",
       "4    [@jkenney, planning, function, vaccine, covid,...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se procesan los tweets\n",
    "processed_tweets = data['tweet'].map(preprocess_tweet)\n",
    "processed_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0c4c3599-cd43-4e1c-acca-e59899480f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 biden\n",
      "1 blame\n",
      "2 covid\n",
      "3 fauci\n",
      "4 governors\n",
      "5 responsibility\n",
      "Largo del diccionario:  47081\n"
     ]
    }
   ],
   "source": [
    "# Se crea el vocabulario\n",
    "dictionary = gensim.corpora.Dictionary(processed_tweets)\n",
    "\n",
    "c = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k,v)\n",
    "    c += 1\n",
    "    if c > 5: break\n",
    "        \n",
    "print('Largo del diccionario: ', len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "92087c2f-1d20-4c4d-ad8f-12cb55c1a21e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1757, 1), (2156, 1), (7554, 1), (14469, 1), (14470, 1)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se transforman las palabras a vectores con el dictionary\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_tweets]\n",
    "bow_corpus[3636]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "561ad5fe-5690-4bf0-aba8-0be850b68f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea y usa el lda model\n",
    "# Running LDA using Bag of Words\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, \\\n",
    "                                    num_topics=5,\\\n",
    "                                    id2word=dictionary,\\\n",
    "                                    passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "51b47375-db79-4e2a-b078-d317c5169583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.011*\"basura\" + 0.009*\"(10-2)\" + 0.008*\"fuego\" + 0.005*\"avenida\" + 0.003*\"jajajaja\" + 0.003*\"pastizales\" + 0.003*\"pasto\" + 0.003*\"santiago\" + 0.003*\"gracias\" + 0.002*\"2021\"\n",
      "Topic: 1 \n",
      "Words: 0.018*\"feliz\" + 0.013*\"nuevo\" + 0.009*\"gracias\" + 0.008*\"fuego\" + 0.008*\"psje\" + 0.006*\"humo\" + 0.006*\"estructura\" + 0.005*\"2021\" + 0.005*\"mejor\" + 0.005*\"llamado\"\n",
      "Topic: 2 \n",
      "Words: 0.006*\"feliz\" + 0.005*\"2020\" + 0.004*\"2021\" + 0.004*\"chile\" + 0.004*\"bueno\" + 0.003*\"quiero\" + 0.003*\"felicidades\" + 0.003*\"nuevo\" + 0.002*\"pedro\" + 0.002*\"covid\"\n",
      "Topic: 3 \n",
      "Words: 0.008*\"avenida\" + 0.007*\"chile\" + 0.006*\"santiago\" + 0.006*\"2021\" + 0.006*\"2020\" + 0.005*\"emanaci√≥n\" + 0.004*\"(10-6)\" + 0.004*\"accidente\" + 0.004*\"colo\" + 0.004*\"vehicular\"\n",
      "Topic: 4 \n",
      "Words: 0.024*\"chile\" + 0.020*\"2021\" + 0.018*\"santiago\" + 0.015*\"feliz\" + 0.012*\"foto\" + 0.011*\"acaba\" + 0.011*\"publicar\" + 0.007*\"nuevo\" + 0.005*\"fuegos\" + 0.005*\"artificiales\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2017108d-e525-4ee3-b973-3e1d9658eb88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f26d05d-c003-4d26-b198-5613dd126bac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
