\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introducción}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Análisis Exploratorio}{1}{section.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig: twint_example1_informe}{{1a}{1}{Subfigure 1a}{subfigure.1.1}{}}
\newlabel{sub@fig: twint_example1_informe}{{(a)}{a}{Subfigure 1a\relax }{subfigure.1.1}{}}
\newlabel{fig: twint_example2_informe}{{1b}{1}{Subfigure 1b}{subfigure.1.2}{}}
\newlabel{sub@fig: twint_example2_informe}{{(b)}{b}{Subfigure 1b\relax }{subfigure.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A la izquierda se buscan y guardan tweets creados al rededor de Santiago entre las fechas \texttt  {since} y \texttt  {until} que además contengan la palabra \texttt  {word}. A la derecha se obtienen y guardan los tweets de la cuenta \texttt  {id}. \textbf  {Scrapping tweets} es la expresión en inglés para este proceso.\relax }}{1}{figure.caption.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{1}{subfigure.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{1}{subfigure.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Wordclouds}{2}{subsection.2.1}\protected@file@percent }
\newlabel{fig: estallido}{{2a}{2}{Subfigure 2a}{subfigure.2.1}{}}
\newlabel{sub@fig: estallido}{{(a)}{a}{Subfigure 2a\relax }{subfigure.2.1}{}}
\newlabel{fig: mujer}{{2b}{2}{Subfigure 2b}{subfigure.2.2}{}}
\newlabel{sub@fig: mujer}{{(b)}{b}{Subfigure 2b\relax }{subfigure.2.2}{}}
\newlabel{fig: pinera}{{2c}{2}{Subfigure 2c}{subfigure.2.3}{}}
\newlabel{sub@fig: pinera}{{(c)}{c}{Subfigure 2c\relax }{subfigure.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces De izquierda a derecha: estallido social, día de la mujer y tweets que contienen la palabra ``piñera''.\relax }}{2}{figure.caption.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{2}{subfigure.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{2}{subfigure.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{2}{subfigure.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Series de tiempo}{2}{subsection.2.2}\protected@file@percent }
\newlabel{fig: serie_covid_2020}{{3a}{3}{Subfigure 3a}{subfigure.3.1}{}}
\newlabel{sub@fig: serie_covid_2020}{{(a)}{a}{Subfigure 3a\relax }{subfigure.3.1}{}}
\newlabel{fig: serie_pinera_2020}{{3b}{3}{Subfigure 3b}{subfigure.3.2}{}}
\newlabel{sub@fig: serie_pinera_2020}{{(b)}{b}{Subfigure 3b\relax }{subfigure.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Cantidad de tweets diarios con la palabra ``covid'' (a) y ``piñera'' (b).\relax }}{3}{figure.caption.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{3}{subfigure.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{3}{subfigure.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Latent Dirichlet Allocation aplicado a datos de Twitter}{3}{section.3}\protected@file@percent }
\citation{LDA03}
\citation{LDA10}
\citation{LDA10}
\citation{coherence}
\citation{coherence2}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Datos}{4}{subsection.3.1}\protected@file@percent }
\newlabel{fig: boric_tweet_año}{{4a}{5}{Subfigure 4a}{subfigure.4.1}{}}
\newlabel{sub@fig: boric_tweet_año}{{(a)}{a}{Subfigure 4a\relax }{subfigure.4.1}{}}
\newlabel{fig: tabla_boric}{{4b}{5}{Subfigure 4b}{subfigure.4.2}{}}
\newlabel{sub@fig: tabla_boric}{{(b)}{b}{Subfigure 4b\relax }{subfigure.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Tweets por año (a) y estadísticas de la cantidad diaria de tweets (b).\relax }}{5}{figure.caption.4}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{5}{subfigure.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{5}{subfigure.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Limpieza y preprocesamiento de datos}{5}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Tamaño del diccionario en función de la cantidad de tweets mínima a la que las palabras deben permanecer. Es decir, $y$ corresponde a las palabras que resultan al filtrar las que aparezcan en menos de $x$ tweets. Este gráfico es importante porque nos permite ver con que tanto vocabulario nos quedamos cuando eliminamos palabras sin mucha importancia o para el modelo.\relax }}{5}{figure.caption.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Estado final de dos tweets escogidos al azar. Esta representación se conoce como ``bow'' (\textbf  {bag of words}).\relax }}{6}{figure.caption.6}\protected@file@percent }
\newlabel{fig: bow_example}{{6}{6}{Estado final de dos tweets escogidos al azar. Esta representación se conoce como \comillas {bow} (\textbf {bag of words}).\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Aplicando el modelo}{6}{subsection.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \texttt  {corpus} corresponde a lo mostrado en la Figura \ref  {fig: bow_example}, \texttt  {id2word} es nuestro vocabulario $V$, \texttt  {alpha} y \texttt  {eta} son los priors discutidos previamente, \texttt  {num\_topics} es la cantidad de tópicos que estamos buscando y \texttt  {random\_state} es la semilla del algoritmo que en los experimentos siempre fue $1$. El método \texttt  {top\_topics} permite obtener coherencia de cada tópico.\relax }}{6}{figure.caption.7}\protected@file@percent }
\newlabel{fig: gensim_code_informe}{{7}{6}{\texttt {corpus} corresponde a lo mostrado en la Figura \ref {fig: bow_example}, \texttt {id2word} es nuestro vocabulario $V$, \texttt {alpha} y \texttt {eta} son los priors discutidos previamente, \texttt {num\_topics} es la cantidad de tópicos que estamos buscando y \texttt {random\_state} es la semilla del algoritmo que en los experimentos siempre fue $1$. El método \texttt {top\_topics} permite obtener coherencia de cada tópico.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Primera iteración}{7}{subsubsection.3.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Los tópicos están ordenados por coherencia y los números que aparecen al lado de cada palabra corresponde a la probabilidad de tal palabra en el tópico. Notar que sólo se muestran las $10$ palabras con mayor probabilidad dentro del tópico, y dado el tamaño de $V$ en este experimento, tales probabilidades son relativamente pequeñas. La coherencia promedio de los tópicos de esta imagen es $-9.1$.\relax }}{7}{figure.caption.8}\protected@file@percent }
\newlabel{fig: boric0_informe}{{8}{7}{Los tópicos están ordenados por coherencia y los números que aparecen al lado de cada palabra corresponde a la probabilidad de tal palabra en el tópico. Notar que sólo se muestran las $10$ palabras con mayor probabilidad dentro del tópico, y dado el tamaño de $V$ en este experimento, tales probabilidades son relativamente pequeñas. La coherencia promedio de los tópicos de esta imagen es $-9.1$.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Segunda iteración}{7}{subsubsection.3.3.2}\protected@file@percent }
\newlabel{fig: param_optimization0_informe}{{9a}{7}{Subfigure 9a}{subfigure.9.1}{}}
\newlabel{sub@fig: param_optimization0_informe}{{(a)}{a}{Subfigure 9a\relax }{subfigure.9.1}{}}
\newlabel{fig: param_optimization2_informe}{{9b}{7}{Subfigure 9b}{subfigure.9.2}{}}
\newlabel{sub@fig: param_optimization2_informe}{{(b)}{b}{Subfigure 9b\relax }{subfigure.9.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Coherencia por cada combinación de parámetros. Las tuplas del eje $x$ son de la forma $(\texttt  {num\_topics}, \alpha , \eta )$. Recordar que esta métrica es tal que mientras más cercana a $0$ mejor. (a) corresponde a la segunda iteración y (b) a la tercera.\relax }}{7}{figure.caption.9}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{7}{subfigure.9.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{7}{subfigure.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Tercera iteración}{8}{subsubsection.3.3.3}\protected@file@percent }
\newlabel{fig: param_optimization3_informe}{{10a}{8}{Subfigure 10a}{subfigure.10.1}{}}
\newlabel{sub@fig: param_optimization3_informe}{{(a)}{a}{Subfigure 10a\relax }{subfigure.10.1}{}}
\newlabel{fig: boric3_informe}{{10b}{8}{Subfigure 10b}{subfigure.10.2}{}}
\newlabel{sub@fig: boric3_informe}{{(b)}{b}{Subfigure 10b\relax }{subfigure.10.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Coherencia (a) y tópicos asociados a los mejores parámetros encontrados (b).\relax }}{8}{figure.caption.10}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{8}{subfigure.10.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{8}{subfigure.10.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4}Cuarta iteración (cambio de data set)}{8}{subsubsection.3.3.4}\protected@file@percent }
\bibcite{LDA03}{1}
\newlabel{fig: mix_param_optimization0_informe}{{11a}{9}{Subfigure 11a}{subfigure.11.1}{}}
\newlabel{sub@fig: mix_param_optimization0_informe}{{(a)}{a}{Subfigure 11a\relax }{subfigure.11.1}{}}
\newlabel{fig: mix0_informe}{{11b}{9}{Subfigure 11b}{subfigure.11.2}{}}
\newlabel{sub@fig: mix0_informe}{{(b)}{b}{Subfigure 11b\relax }{subfigure.11.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Coherencia (a) y tópicos asociados a los mejores parámetros encontrados (b).\relax }}{9}{figure.caption.11}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{9}{subfigure.11.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{9}{subfigure.11.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusión}{9}{section.4}\protected@file@percent }
\bibcite{LDA10}{2}
\bibcite{coherence}{3}
\bibcite{coherence2}{4}
\gdef \@abspage@last{11}
