\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Análisis Exploratorio}{1}{section.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig: twint_example1_informe}{{1a}{1}{Subfigure 1a}{subfigure.1.1}{}}
\newlabel{sub@fig: twint_example1_informe}{{(a)}{a}{Subfigure 1a\relax }{subfigure.1.1}{}}
\newlabel{fig: twint_example2_informe}{{1b}{1}{Subfigure 1b}{subfigure.1.2}{}}
\newlabel{sub@fig: twint_example2_informe}{{(b)}{b}{Subfigure 1b\relax }{subfigure.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A la izquierda se buscan y guardan tweets creados al rededor de Santiago entre las fechas \texttt  {since} y \texttt  {until} que además contengan la palabra \texttt  {word}. A la derecha se obtienen y guardan los tweets de la cuenta \texttt  {id}. \textbf  {Scrapping tweets} es la expresión en inglés para este proceso.\relax }}{1}{figure.caption.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{1}{subfigure.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{1}{subfigure.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Wordclouds}{1}{subsection.1.1}\protected@file@percent }
\newlabel{fig: estallido}{{2a}{2}{Subfigure 2a}{subfigure.2.1}{}}
\newlabel{sub@fig: estallido}{{(a)}{a}{Subfigure 2a\relax }{subfigure.2.1}{}}
\newlabel{fig: mujer}{{2b}{2}{Subfigure 2b}{subfigure.2.2}{}}
\newlabel{sub@fig: mujer}{{(b)}{b}{Subfigure 2b\relax }{subfigure.2.2}{}}
\newlabel{fig: pinera}{{2c}{2}{Subfigure 2c}{subfigure.2.3}{}}
\newlabel{sub@fig: pinera}{{(c)}{c}{Subfigure 2c\relax }{subfigure.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces De izquierda a derecha: estallido social, día de la mujer y tweets que contienen la palabra ``piñera''.\relax }}{2}{figure.caption.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{2}{subfigure.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{2}{subfigure.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{2}{subfigure.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Series de tiempo}{2}{subsection.1.2}\protected@file@percent }
\newlabel{fig: serie_covid_2020}{{3a}{2}{Subfigure 3a}{subfigure.3.1}{}}
\newlabel{sub@fig: serie_covid_2020}{{(a)}{a}{Subfigure 3a\relax }{subfigure.3.1}{}}
\newlabel{fig: serie_pinera_2020}{{3b}{2}{Subfigure 3b}{subfigure.3.2}{}}
\newlabel{sub@fig: serie_pinera_2020}{{(b)}{b}{Subfigure 3b\relax }{subfigure.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Cantidad de tweets diarios con la palabra ``covid'' (izq) y ``piñera'' (der).\relax }}{2}{figure.caption.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{2}{subfigure.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{2}{subfigure.3.2}\protected@file@percent }
\citation{LDA03}
\citation{LDA10}
\citation{LDA10}
\@writefile{toc}{\contentsline {section}{\numberline {2}Latent Dirichlet Allocation aplicado a datos de Twitter}{4}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Datos}{4}{subsection.2.1}\protected@file@percent }
\newlabel{fig: boric_tweet_año}{{4a}{5}{Subfigure 4a}{subfigure.4.1}{}}
\newlabel{sub@fig: boric_tweet_año}{{(a)}{a}{Subfigure 4a\relax }{subfigure.4.1}{}}
\newlabel{fig: boric_tweet_2020}{{4b}{5}{Subfigure 4b}{subfigure.4.2}{}}
\newlabel{sub@fig: boric_tweet_2020}{{(b)}{b}{Subfigure 4b\relax }{subfigure.4.2}{}}
\newlabel{fig: tabla_boric}{{4c}{5}{Subfigure 4c}{subfigure.4.3}{}}
\newlabel{sub@fig: tabla_boric}{{(c)}{c}{Subfigure 4c\relax }{subfigure.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Tweets por año (a) y por mes en el $2020$ (b). Tamaño del data set y estadísticas de la cantidad diaria de tweets (c).\relax }}{5}{figure.caption.4}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{5}{subfigure.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{5}{subfigure.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{5}{subfigure.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Limpieza y preprocesamiento de datos}{5}{subsection.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Tamaño del diccionario en función de la cantidad de tweets mínima a la que las palabras deben permanecer. Es decir, $y$ corresponde a las palabras que resultan al filtrar las que aparezcan en menos de $x$ tweets. Este gráfico es importante porque nos permite ver con que tanto vocabulario nos quedamos cuando eliminamos palabras sin mucha importancia o para el modelo.\relax }}{6}{figure.caption.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Estado final de dos tweets escogidos al azar. Esta representación se conoce como ``bow'' (\textbf  {bag of words}).\relax }}{6}{figure.caption.6}\protected@file@percent }
\newlabel{fig: bow_example}{{6}{6}{Estado final de dos tweets escogidos al azar. Esta representación se conoce como \comillas {bow} (\textbf {bag of words}).\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Aplicando el modelo}{7}{subsection.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \texttt  {corpus} corresponde a lo mostrado en la Figura \ref  {fig: bow_example}, \texttt  {id2word} es nuestro vocabulario $V$, \texttt  {alpha} y \texttt  {eta} son los priors discutidos previamente, \texttt  {num\_topics} es la cantidad de tópicos que estamos buscando y \texttt  {random\_state} es la semilla del algoritmo. El método \texttt  {top\_topics} permite obtener coherencia de cada tópico.\relax }}{7}{figure.caption.7}\protected@file@percent }
\newlabel{fig: gensim_code_informe}{{7}{7}{\texttt {corpus} corresponde a lo mostrado en la Figura \ref {fig: bow_example}, \texttt {id2word} es nuestro vocabulario $V$, \texttt {alpha} y \texttt {eta} son los priors discutidos previamente, \texttt {num\_topics} es la cantidad de tópicos que estamos buscando y \texttt {random\_state} es la semilla del algoritmo. El método \texttt {top\_topics} permite obtener coherencia de cada tópico.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Primera iteración}{7}{subsubsection.2.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Los tópicos están ordenados por coherencia y los números que aparecen al lado de cada palabra corresponde a la probabilidad de tal palabra en el tópico. Notar que sólo se muestran las $10$ palabras con mayor probabilidad dentro del tópico, y dado el tamaño de $V$ en este experimento, tales probabilidades son relativamente pequeñas. La coherencia promedio de los tópicos de esta imagen es $-9.1$.\relax }}{7}{figure.caption.8}\protected@file@percent }
\newlabel{fig: boric0_informe}{{8}{7}{Los tópicos están ordenados por coherencia y los números que aparecen al lado de cada palabra corresponde a la probabilidad de tal palabra en el tópico. Notar que sólo se muestran las $10$ palabras con mayor probabilidad dentro del tópico, y dado el tamaño de $V$ en este experimento, tales probabilidades son relativamente pequeñas. La coherencia promedio de los tópicos de esta imagen es $-9.1$.\relax }{figure.caption.8}{}}
\bibcite{LDA03}{1}
\bibcite{LDA10}{2}
\bibcite{coherence}{3}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Segunda iteración}{8}{subsubsection.2.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Coherencia por cada combinación de parámetros. Las tuplas del eje $x$ son de la forma $(\texttt  {num\_topics}, \alpha , \eta )$.\relax }}{8}{figure.caption.9}\protected@file@percent }
\newlabel{fig: param_optimization0_informe}{{9}{8}{Coherencia por cada combinación de parámetros. Las tuplas del eje $x$ son de la forma $(\texttt {num\_topics}, \alpha , \eta )$.\relax }{figure.caption.9}{}}
\gdef \@abspage@last{9}
